{"meta":{"title":"Derek Elliott Blog","subtitle":null,"description":null,"author":"Derek Elliott","url":"http://derekelliott.me"},"pages":[],"posts":[{"title":"Santander Customer Satisfaction - Random Forest Classifier","slug":"Santander-Customer-Satisfaction-Random-Forest-Classifier","date":"2016-12-04T02:43:30.000Z","updated":"2016-12-04T03:24:08.715Z","comments":true,"path":"2016/12/03/Santander-Customer-Satisfaction-Random-Forest-Classifier/","link":"","permalink":"http://derekelliott.me/2016/12/03/Santander-Customer-Satisfaction-Random-Forest-Classifier/","excerpt":"","text":"This is the second in a series of post detailing the analysis I’ve performed on the Santander Customer Satisfaction competition on Kaggle. This is the first algorithm that I tried to tune the parameters on my own. I couldn’t find a good guide, so I did my best based on the guide I used for the gradient boosted algorithm. I used the same helper function, modified slightly to fit the algorithm. 12345678910111213141516171819202122232425def modelFit(alg, dtrain, predictors, target, performCV = True, printFeatReport = True, cv_folds = 5): alg.fit(dtrain[predictors], dtrain[target]) dtrain_predictions = alg.predict(dtrain[predictors]) dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1] if performCV: cv_score = cross_validation.cross_val_score(alg, dtrain[predictors], dtrain[target], \\ cv = cv_folds, scoring = 'roc_auc' ) print('\\nModel Report') print('Accuracy : &#123;:.4g&#125;'.format(metrics.accuracy_score(dtrain[target].values, dtrain_predictions))) print('AUC Score (Train): &#123;:f&#125;'.format(metrics.roc_auc_score(dtrain[target], dtrain_predprob))) if performCV: print('CV Score: Mean - &#123;:.7g&#125; | Std - &#123;:.7g&#125; | Min - &#123;:.7g&#125; | Max - &#123;:.7g&#125;'.format(np.mean(cv_score),\\ np.std(cv_score),\\ np.min(cv_score),\\ np.max(cv_score))) if printFeatReport: feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending = False) feat_imp.plot(kind = 'bar', title = 'Feature Importances') plt.ylabel('Feature Importance Score') Along with what I mentioned last time, I think I want to modify this to automatically remove features that don’t contribute to the model. I could do it iteratively and stop when the cross validation score starts to drop. These models already take a few minutes each to run, and when I start to cross validate, it could take 10 or 15 minutes to train. In the grand scheme of things, that’s not that long considering what it could take with more advanced models, but if I start trying to ensemble I’d rather not have to wait overnight to check my results. I use the exact same code to import and clean the data to get it ready for training the model, but I’ll include it below for the sake of completeness. 12345678910111213141516171819202122232425262728293031train = pd.read_csv('Data/Santander/train.csv')test = pd.read_csv('Data/Santander/test.csv')dropCols = []for i in train.columns.values: if len(train[i].unique()) == 1: dropCols.append(i)print('Dropping &#123;&#125; columns due to non-unique entries'.format(len(dropCols)))train.drop(dropCols, axis = 1, inplace = True)test.drop(dropCols, axis = 1, inplace = True)dropCols = []c = train.columnsfor i in range(len(c)-1): v = train[c[i]].values for j in range(i+1,len(c)): if np.array_equal(v,train[c[j]].values): dropCols.append(c[j])print('Dropping &#123;&#125; columns due to duplicate columns'.format(len(dropCols)))train.drop(dropCols, axis = 1, inplace = True)test.drop(dropCols, axis = 1, inplace = True)train.loc[train.var3 &lt; -10000, 'var3'] = np.nantest.loc[test.var3 &lt; -10000, 'var3'] = np.nancolsWithNAN = []for i in train.columns.values: if train[i].isnull().sum() &gt; 0: colsWithNAN.append(i)train.drop(colsWithNAN, axis = 1, inplace = True)test.drop(colsWithNAN, axis = 1, inplace = True) I ran the model with the default parameter values and set the random state so that I could train multiple times and get consistent results. 1234pred = [i for i in train.columns if i not in ['ID', 'TARGET']]rfc0 = RandomForestClassifier(n_estimators = 500, \\ random_state = 42)modelFit(rfc0, train, pred, 'TARGET') The default parameters resulted in a CV score of 0.7653. Not bad for a first run, but there is a lot of room for improvement. Next, instead of running several different grid searches on each parameter, I ran a randomized search on all of the relevant parameters. The function to report the findings of the random search was taken from the scikit-learn example. It was modified very slightly to fit my needs though. 1234567891011121314151617def report(grid_scores, n_top=3): top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top] for i, score in enumerate(top_scores): print(\"\\nModel with rank: &#123;0&#125;\".format(i + 1)) print(\"Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)\".format(score.mean_validation_score,np.std(score.cv_validation_scores))) print(\"Parameters: &#123;0&#125;\".format(score.parameters))param_dist = &#123;\"max_depth\": sp_randint(3,10), \"max_features\": sp_randint(15, 29), \"min_samples_split\": sp_randint(400, 600), \"min_samples_leaf\": sp_randint(40, 60), \"bootstrap\": [True, False], \"criterion\": [\"gini\", \"entropy\"]&#125;random_search = RandomizedSearchCV(rfc0, param_distributions = param_dist, n_iter = 20)random_search.fit(train[pred], train['TARGET']) After running it, it produced the following output: 123456789101112Model with rank: 1Mean validation score: 0.960 (std: 0.000)Parameters: &#123;'bootstrap': True, 'min_samples_leaf': 45, 'min_samples_split': 404, 'criterion': 'gini', 'max_features': 24, 'max_depth': 8&#125;Model with rank: 2Mean validation score: 0.960 (std: 0.000)Parameters: &#123;'bootstrap': False, 'min_samples_leaf': 45, 'min_samples_split': 554, 'criterion': 'gini', 'max_features': 24, 'max_depth': 6&#125;Model with rank: 3Mean validation score: 0.960 (std: 0.000)Parameters: &#123;'bootstrap': True, 'min_samples_leaf': 52, 'min_samples_split': 597, 'criterion': 'entropy', 'max_features': 28, 'max_depth': 4&#125;report(random_search.grid_scores_) Then, I created a model with the best parameters and trained it to check out the cross validation score. 12345678rfc1 = RandomForestClassifier(n_estimators = 500, \\ max_depth = 8, \\ max_features = 24, \\ min_samples_split = 404, \\ min_samples_leaf = 45, \\ bootstrap = True, \\ random_state = 42)modelFit(rfc1, train, pred, 'TARGET') This lead to a CV score of 0.8143. That’s an increase of almost 0.05. That’s a substantial improvement over the default parameters. This is as far as I took this model, but in the future, I plan on following up with a few grid searches to further refine the parameters. This is because of the way the RandomizedSearchCV searches for the best parameters. It only selects a sample from the given range, so while what it found is an improvement, it might not have provided the optimal parameters. That’s it for this edition. The next post will detail how I used the XGBoost model to fit the same data. That’s all I already have prepared, so following that, it’s hard to say what will be coming. I might go back and do the things I’ve already suggested with the models I already have, or I’ll start to ensemble. I also need to put up something on the initial data exploration and do some visualizations. So, stay tuned. You can find the notebook on my GitHub, but as I work on this model and the others, it’s subject to change.","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"http://derekelliott.me/categories/Data-Analysis/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://derekelliott.me/tags/Python/"},{"name":"Data Analysis","slug":"Data-Analysis","permalink":"http://derekelliott.me/tags/Data-Analysis/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://derekelliott.me/tags/Kaggle/"}]},{"title":"Santander Customer Satisfaction - Gradient Boosted Classifier","slug":"Santander-Customer-Satisfaction-Gradient-Boosted-Classifier","date":"2016-12-04T02:43:01.000Z","updated":"2016-12-04T03:24:29.308Z","comments":true,"path":"2016/12/03/Santander-Customer-Satisfaction-Gradient-Boosted-Classifier/","link":"","permalink":"http://derekelliott.me/2016/12/03/Santander-Customer-Satisfaction-Gradient-Boosted-Classifier/","excerpt":"","text":"I started this blog to show off some of the things I’m doing to learn more about data analysis. So, let’s get started. Recently, I’ve been looking at the Santander Customer Satisfaction competition on Kaggle. I started off with gradient boosting classifier. I took a lot of inspiration from Analytics Vidhya. He has some really good guides on parameter tuning in both Python and R. First off, I made all the required imports and then adapted a function from the previous website to help check out the performance of the model. 123456789101112131415161718192021222324252627282930def modelFit(alg, dtrain, predictors, target, performCV = True, printFeatReport = True, cv_folds = 5):#alg is the model to fit, dtrain is the training dataframe, predictors is a string or list of#strings of the column names to use as predictors, target a string of the column with the target#performCV will crossvalidate the model, printFeatImport will print a graph showing the important#features, and cv_folds is the number of folds to use in cross validation.'''alg.fit(dtrain[predictors], dtrain[target])dtrain_predictions = alg.predict(dtrain[predictors])dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]if performCV: cv_score = cross_validation.cross_val_score(alg, dtrain[predictors], dtrain[target], \\ cv = cv_folds, scoring = 'roc_auc' )print('\\nModel Report')print('Accuracy : &#123;:.4g&#125;'.format(metrics.accuracy_score(dtrain[target].values, dtrain_predictions)))print('AUC Score (Train): &#123;:f&#125;'.format(metrics.roc_auc_score(dtrain[target], dtrain_predprob)))if performCV: print('CV Score: Mean - &#123;:.7g&#125; | Std - &#123;:.7g&#125; | Min - &#123;:.7g&#125; | Max - &#123;:.7g&#125;'.format(np.mean(cv_score),\\ np.std(cv_score),\\ np.min(cv_score),\\ np.max(cv_score)))if printFeatReport: feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending = False) feat_imp.plot(kind = 'bar', title = 'Feature Importances') plt.ylabel('Feature Importance Score') I haven’t been working with it long, but there are already a few tweeks that I’d like to make. Mainly, to list out the top performing features so I can start thinking about cutting down on the features I’m training with. I import the data, and do some very minor cleaning. I first drop the columns that are uniform, i.e. have zero variance, then I drop columns that are identical. Then, there is only one column of the data that has farily obvious outliers and several invalid entries. Discounting the outliers and NaN’s, the column has so little variance that instead of working with it futher, I didn’t use it to train the model. All told, 63 columns were removed from the training and testing data. 12345678910111213141516171819202122232425262728293031train = pd.read_csv('Data/Santander/train.csv')test = pd.read_csv('Data/Santander/test.csv')dropCols = []for i in train.columns.values: if len(train[i].unique()) == 1: dropCols.append(i)print('Dropping &#123;&#125; columns due to non-unique entries'.format(len(dropCols)))train.drop(dropCols, axis = 1, inplace = True)test.drop(dropCols, axis = 1, inplace = True)dropCols = []c = train.columnsfor i in range(len(c)-1): v = train[c[i]].values for j in range(i+1,len(c)): if np.array_equal(v,train[c[j]].values): dropCols.append(c[j])print('Dropping &#123;&#125; columns due to duplicate columns'.format(len(dropCols)))train.drop(dropCols, axis = 1, inplace = True)test.drop(dropCols, axis = 1, inplace = True)train.loc[train.var3 &lt; -10000, 'var3'] = np.nantest.loc[test.var3 &lt; -10000, 'var3'] = np.nancolsWithNAN = []for i in train.columns.values: if train[i].isnull().sum() &gt; 0: colsWithNAN.append(i)train.drop(colsWithNAN, axis = 1, inplace = True)test.drop(colsWithNAN, axis = 1, inplace = True) Next, I trained the model with all of the default values to get a baseline for the models performance. 123pred = [i for i in train.columns if i not in ['ID', 'TARGET']]gbm0 = GradientBoostingClassifier(random_state = 10)modelFit(gbm0, train, pred, 'TARGET', printFeatReport = False) The results it gave me were decent for a first run. The AUC score was around .85 and it cross validated at .8352. Next, I ran a few different grid searches and refined the model. 123456789101112131415161718192021paramTest1 = &#123;'n_estimators' : range(80,141,10)&#125;gSearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate = 0.1,\\ min_samples_split = 500,\\ min_samples_leaf = 50,\\ max_depth = 8,\\ max_features = 'sqrt',\\ subsample = 0.8,\\ random_state = 10), param_grid = paramTest1, scoring = 'roc_auc', n_jobs = 8, iid = False, cv = 5)gSearch1.fit(train[pred], train['TARGET'])gSearch1.grid_scores_, gSearch1.best_params_, gSearch1.best_score_([mean: 0.83770, std: 0.00833, params: &#123;'n_estimators': 80&#125;, mean: 0.83784, std: 0.00819, params: &#123;'n_estimators': 90&#125;, mean: 0.83793, std: 0.00867, params: &#123;'n_estimators': 100&#125;, mean: 0.83757, std: 0.00844, params: &#123;'n_estimators': 110&#125;, mean: 0.83734, std: 0.00855, params: &#123;'n_estimators': 120&#125;, mean: 0.83726, std: 0.00826, params: &#123;'n_estimators': 130&#125;, mean: 0.83698, std: 0.00809, params: &#123;'n_estimators': 140&#125;], &#123;'n_estimators': 100&#125;, 0.83792968856907224) First I checked the number of estimators, and as you can see from above, it found that 100 was the best fit. 12345678910111213141516171819202122232425262728293031323334353637383940414243paramTest2 = &#123;'max_depth' : range(5,16,2), 'min_samples_split' : range(200,1001, 200)&#125;gSearch2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate = 0.1,\\ n_estimators = 100,\\ min_samples_leaf = 50,\\ max_features = 'sqrt',\\ subsample = 0.8,\\ random_state = 10), param_grid = paramTest2, scoring = 'roc_auc', n_jobs = 8, iid = False, cv = 5)gSearch2.fit(train[pred], train['TARGET'])gSearch2.grid_scores_, gSearch2.best_params_, gSearch2.best_score_([mean: 0.83576, std: 0.00915, params: &#123;'min_samples_split': 200, 'max_depth': 5&#125;, mean: 0.83705, std: 0.00813, params: &#123;'min_samples_split': 400, 'max_depth': 5&#125;, mean: 0.83613, std: 0.00887, params: &#123;'min_samples_split': 600, 'max_depth': 5&#125;, mean: 0.83639, std: 0.00912, params: &#123;'min_samples_split': 800, 'max_depth': 5&#125;, mean: 0.83573, std: 0.00873, params: &#123;'min_samples_split': 1000, 'max_depth': 5&#125;, mean: 0.83491, std: 0.00716, params: &#123;'min_samples_split': 200, 'max_depth': 7&#125;, mean: 0.83665, std: 0.00764, params: &#123;'min_samples_split': 400, 'max_depth': 7&#125;, mean: 0.83653, std: 0.00902, params: &#123;'min_samples_split': 600, 'max_depth': 7&#125;, mean: 0.83680, std: 0.00873, params: &#123;'min_samples_split': 800, 'max_depth': 7&#125;, mean: 0.83667, std: 0.00902, params: &#123;'min_samples_split': 1000, 'max_depth': 7&#125;, mean: 0.83550, std: 0.00747, params: &#123;'min_samples_split': 200, 'max_depth': 9&#125;, mean: 0.83760, std: 0.00702, params: &#123;'min_samples_split': 400, 'max_depth': 9&#125;, mean: 0.83716, std: 0.00751, params: &#123;'min_samples_split': 600, 'max_depth': 9&#125;, mean: 0.83675, std: 0.00851, params: &#123;'min_samples_split': 800, 'max_depth': 9&#125;, mean: 0.83663, std: 0.00870, params: &#123;'min_samples_split': 1000, 'max_depth': 9&#125;, mean: 0.83276, std: 0.00819, params: &#123;'min_samples_split': 200, 'max_depth': 11&#125;, mean: 0.83558, std: 0.00950, params: &#123;'min_samples_split': 400, 'max_depth': 11&#125;, mean: 0.83589, std: 0.00749, params: &#123;'min_samples_split': 600, 'max_depth': 11&#125;, mean: 0.83713, std: 0.00861, params: &#123;'min_samples_split': 800, 'max_depth': 11&#125;, mean: 0.83527, std: 0.00898, params: &#123;'min_samples_split': 1000, 'max_depth': 11&#125;, mean: 0.83208, std: 0.00747, params: &#123;'min_samples_split': 200, 'max_depth': 13&#125;, mean: 0.83346, std: 0.00818, params: &#123;'min_samples_split': 400, 'max_depth': 13&#125;, mean: 0.83536, std: 0.00780, params: &#123;'min_samples_split': 600, 'max_depth': 13&#125;, mean: 0.83700, std: 0.00883, params: &#123;'min_samples_split': 800, 'max_depth': 13&#125;, mean: 0.83624, std: 0.00855, params: &#123;'min_samples_split': 1000, 'max_depth': 13&#125;, mean: 0.83061, std: 0.00964, params: &#123;'min_samples_split': 200, 'max_depth': 15&#125;, mean: 0.83242, std: 0.00796, params: &#123;'min_samples_split': 400, 'max_depth': 15&#125;, mean: 0.83445, std: 0.00908, params: &#123;'min_samples_split': 600, 'max_depth': 15&#125;, mean: 0.83434, std: 0.00896, params: &#123;'min_samples_split': 800, 'max_depth': 15&#125;, mean: 0.83583, std: 0.00880, params: &#123;'min_samples_split': 1000, 'max_depth': 15&#125;], &#123;'max_depth': 9, 'min_samples_split': 400&#125;, 0.83759736130706453) Next, I searched for the max_depth and min_samples_split parameters. As you can see, the best values were 9 and 400 respectively. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162paramTest3 = &#123;'min_samples_leaf' : range(30,71, 10)&#125;gSearch3 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate = 0.1,\\ n_estimators = 100,\\ max_depth = 9,\\ min_samples_split = 400,\\ max_features = 'sqrt',\\ subsample = 0.8,\\ random_state = 10), param_grid = paramTest3, scoring = 'roc_auc', n_jobs = 8, iid = False, cv = 5)gSearch3.fit(train[pred], train['TARGET'])gSearch3.grid_scores_, gSearch3.best_params_, gSearch3.best_score_([mean: 0.83655, std: 0.00843, params: &#123;'min_samples_leaf': 30&#125;, mean: 0.83702, std: 0.00998, params: &#123;'min_samples_leaf': 40&#125;, mean: 0.83760, std: 0.00702, params: &#123;'min_samples_leaf': 50&#125;, mean: 0.83666, std: 0.00899, params: &#123;'min_samples_leaf': 60&#125;, mean: 0.83603, std: 0.00967, params: &#123;'min_samples_leaf': 70&#125;], &#123;'min_samples_leaf': 50&#125;, 0.83759736130706453) paramTest4 = &#123;'max_features' : range(17,30, 2)&#125;gSearch4 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate = 0.1,\\ n_estimators = 100,\\ max_depth = 9,\\ min_samples_split = 400,\\ min_samples_leaf = 50,\\ subsample = 0.8,\\ random_state = 10), param_grid = paramTest4, scoring = 'roc_auc', n_jobs = 8, iid = False, cv = 5)gSearch4.fit(train[pred], train['TARGET'])gSearch4.grid_scores_, gSearch4.best_params_, gSearch4.best_score_([mean: 0.83760, std: 0.00702, params: &#123;'max_features': 17&#125;, mean: 0.83686, std: 0.00983, params: &#123;'max_features': 19&#125;, mean: 0.83567, std: 0.00880, params: &#123;'max_features': 21&#125;, mean: 0.83631, std: 0.00850, params: &#123;'max_features': 23&#125;, mean: 0.83735, std: 0.00862, params: &#123;'max_features': 25&#125;, mean: 0.83588, std: 0.00836, params: &#123;'max_features': 27&#125;, mean: 0.83722, std: 0.00796, params: &#123;'max_features': 29&#125;], &#123;'max_features': 17&#125;, 0.83759736130706453) paramTest5 = &#123;'subsample' : [0.6, 0.7, 0.75, 0.8, 0.85, 0.9]&#125;gSearch5 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate = 0.1,\\ n_estimators = 100,\\ max_depth = 9,\\ min_samples_split = 400,\\ min_samples_leaf = 50,\\ max_features = 'sqrt',\\ random_state = 10),\\ param_grid = paramTest5, scoring = 'roc_auc', n_jobs = 8, iid = False, cv = 5)gSearch5.fit(train[pred], train['TARGET'])gSearch5.grid_scores_, gSearch5.best_params_, gSearch5.best_score_([mean: 0.83661, std: 0.00957, params: &#123;'subsample': 0.6&#125;, mean: 0.83638, std: 0.00789, params: &#123;'subsample': 0.7&#125;, mean: 0.83606, std: 0.00882, params: &#123;'subsample': 0.75&#125;, mean: 0.83760, std: 0.00702, params: &#123;'subsample': 0.8&#125;, mean: 0.83678, std: 0.00788, params: &#123;'subsample': 0.85&#125;, mean: 0.83663, std: 0.00895, params: &#123;'subsample': 0.9&#125;], &#123;'subsample': 0.8&#125;, 0.83759736130706453) The final three grid searches I ran were for min_samples_leaf, max_features, and subsample. After tuning these 6 parameters, I got the cross validation score to .8379 or an increase of .002. The following is the final feature importance chart. The next thing I plan to do with this model is reduce the number of features using the above chart and adjust the learning rate. I’m hoping to be able to squeeze a few more hundreths of a point out of this model. This will be an ongoing series, as I have already trained a random forest and used XGBoost to model the same data and will be posting about them in the future. Stay tuned, because eventually I’ll start to ensemble these models and see how high I can finish in the leaderboards. As of writing this, I’m sitting just above the 50th percentile. You can find the notebook file on my Github, but be warned that it is constantly changing as I find time to mess with it.","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"http://derekelliott.me/categories/Data-Analysis/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://derekelliott.me/tags/Python/"},{"name":"Data Analysis","slug":"Data-Analysis","permalink":"http://derekelliott.me/tags/Data-Analysis/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://derekelliott.me/tags/Kaggle/"}]},{"title":"This is just a test","slug":"First-Post","date":"2016-12-04T02:42:36.000Z","updated":"2016-12-04T02:42:36.557Z","comments":true,"path":"2016/12/03/First-Post/","link":"","permalink":"http://derekelliott.me/2016/12/03/First-Post/","excerpt":"","text":"This is an example of a helper function I used when building my XGBoost model for the Kaggle competition, Santander Customer Satisfaction. 12345678910111213141516171819202122def modelFit(alg, dtrain, predictors, target, useTrainCV = True, early_stopping_rounds = 50, cv_folds = 5): '''.''' if useTrainCV: xgb_param = alg.get_xgb_params() xgtrain = xgb.DMatrix(dtrain[predictors].values, label = dtrain[target].values) cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round = alg.get_params()['n_estimators'], nfold = cv_folds,\\ metrics = 'auc', early_stopping_rounds = early_stopping_rounds, verbose_eval = 100) alg.set_params(n_estimators = cvresult.shape[0]) alg.fit(dtrain[predictors], dtrain[target], eval_metric = 'auc') dtrain_predictions = alg.predict(dtrain[predictors]) dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1] print('\\\\nModel Report') print('Accuracy : &#123;:.4g&#125;'.format(metrics.accuracy_score(dtrain[target].values, dtrain_predictions))) print('AUC Score (Train): &#123;:f&#125;'.format(metrics.roc_auc_score(dtrain[target], dtrain_predprob))) feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending = False) feat_imp.plot(kind = 'bar', title = 'Feature Importances') plt.ylabel('Feature Importance Score')\" The End?","categories":[],"tags":[]}]}